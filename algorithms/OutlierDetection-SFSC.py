# -*- coding: utf-8 -*-
"""OutlierDetection-SFSC.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_yoXtchm2nOWxDC-5FFpNuL27HDqWVN

# Importing the libraries
"""

import math
import gc
import time
import numpy as np
from scipy.stats import norm
from sklearn.neighbors import KDTree, BallTree
import time
from sklearn.datasets import make_blobs
import numpy as np
import pandas as pd
from scipy.linalg import sqrtm, null_space, inv
from scipy.sparse.linalg import eigs, LinearOperator
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import networkx as nx
import time
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import SpectralClustering
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""# Gaussian kernel for similarity"""

def get_similarity(point1, point2):
    eu_dist = math.sqrt(np.sum((point1 -point2)**2))
    sim = math.exp(-eu_dist / (2 * 2))
    return sim

"""# Similarity matrix"""

def cal_sim_matrix(X):
    X = np.array(X)
    num_points, d = X.shape
    S = np.zeros((num_points, num_points))

    for i in range(num_points):
        for j in range(i+1, num_points):
            try:
                sum_d = 0
                for k in range(d):
                    dif = (X[i][k] - X[j][k])**2
                    sum_d += dif
                sim = math.exp(-math.sqrt(sum_d) / (2 * 2))
            except:
                sim = 1
                print("error: ", X[i], X[j])
            S[i][j] = sim
            S[j][i] = S[i][j]
    return S

# W = cal_sim_matrix(data1)
# print(W)
# print(W.shape)

"""# Diagonal matrix"""

def cal_diag_matrix(sim_matrix):
  diag_values = np.sum(sim_matrix, axis = 0)
  diag_matrix = np.diag(diag_values)
  return diag_matrix

# D = cal_diag_matrix(W)
# print(D)
# print(D.shape)

"""# Group Indicator Matrix"""

def cal_f_matrix(data, h):
  # data_df = pd.DataFrame(data)
  data = np.array(data)
  class_labels = data[:, 0]
  # unique_classes = np.unique(data[:, 0])
  # h = len(unique_classes)
  F = np.zeros((len(data), h), dtype=int)

  for i, class_label in enumerate(class_labels):
    # class_indices = np.where(data[:, 0] == class_label)[0]
    F[i, class_label - 1] = 1

  F = F[:, :-1]
  return F

def sfsc(mini_dataset, k):

  # start the timer
  start_time = time.time()

  # get W matrix
  W = cal_sim_matrix(mini_dataset)
  print(W)

  # get D matrix
  D = cal_diag_matrix(W)

  # get F matrix
  F = cal_f_matrix(mini_dataset, 4)
  print("Printing F deatils:\n")
  print(F)
  print(F.shape)

  n = W.shape[0]

  # step 1: Find laplacian matrix L
  L = D - W
  print("details of L:")
  print(L.shape)
  # print(L)

  # step 2: Find sqrt of D
  sqrtD = sqrtm(D)
  print("details of sqrt(D):")
  print(D.shape)
  # print(D)

  # step 3: Normalized laplacian matrix Ln
  Ln = np.linalg.inv(sqrtD) @ L @ np.linalg.inv(sqrtD)
  print("details of Ln:")
  print(Ln.shape)
  # print(Ln)

  # step 4: Find C matrix
  C = np.linalg.solve(np.linalg.inv(sqrtD), F)
  print("details of C:")
  print(C.shape)
  # print(C)

  # step 5: find U2 matrix (orthonormal basis of the range of a matrix)
  U2, _ = np.linalg.qr(C)
  print("details of U2:")
  print(U2.shape)
  # print(U2)

  # step 6: find V matrix (orthonormal basis matrix of nullspace of a matrix)
  U, S, Vt = np.linalg.svd(C.T)
  nullspace_basis = Vt.T[:, S.size:].conj()
  V, _ = np.linalg.qr(nullspace_basis)
  print("details of V:")
  print(V.shape)

  # step 7: find P matrix P = VV.T
  P = V @ V.T
  print("details of P:")
  print(P.shape)
  # print(P)

  # step 8: find Lnv matrix Lnv = VTLnV
  Lnv = V.T @ Ln @ V
  print("details of Lnv:")
  print(Lnv.shape)
  # print(Lnv)

  # step 9: find eigenvalues of Lnv and get kth eigenvalue
  Lnv_eigen = np.linalg.eigvals(Lnv)
  Lnv_eigen_sorted = np.sort(Lnv_eigen)
  # print("details of Lnv eigenvalues:")
  # print(Lnv_eigen_sorted)
  if len(Lnv_eigen_sorted) >= k:
    Lnv_eigen_k = Lnv_eigen_sorted[k - 1]
  else:
    Lnv_eigen_k = 0

  # step 10: find sigma
  sigma = Lnv_eigen_k + 0.5
  print("sigma value: ", sigma)

  # step 11: find Lnp matrix Lnp = PLnP
  Lnp = P @ Ln @ P
  print("details of Lnp:")
  print(Lnp.shape)
  # print(Lnp)

  # step 12: find Lnsigma matrix
  # step a : calculate sigma*U2*U2T
  l1 = U2 @ U2.T
  l1 = sigma * l1

  # step b: finding Lnsigma
  Lnsigma = Lnp + l1
  print("details of Lnsigma:")
  print(Lnsigma.shape)
  # print(Lnsigma)

  # step 13: find eigenvalues of Lnsigma
  Lnsigma_eigenvalues, Lnsigma_eigenvectors = np.linalg.eig(Lnsigma)
  sorted_indices = np.argsort(Lnsigma_eigenvalues)  # Sort in ascending order
  Lnsigma_eigenvalues = Lnsigma_eigenvalues[sorted_indices][:k]
  Lnsigma_eigenvectors = Lnsigma_eigenvectors[:, sorted_indices][:, :k]
  print("details of eigenvectors of Lnsigma:")
  print(Lnsigma_eigenvectors.shape)
  print("Printing eigenvalues\n")
  print(Lnsigma_eigenvalues[1])

  # step 14: finding H matrix H = sqrt(D)Lnsigma_eigenvectors
  H = inv(sqrtD) @ Lnsigma_eigenvectors
  H = np.real(H)
  # print("details of H:")
  # print(H.shape)
  # print(H)

  # try:
  #   eigenvalues, _ = np.linalg.eig(np.matmul(H, H.T))
  # # eigenvalues, eigenvectors = np.linalg.eig(H)
  #   print(eigenvalues)
  # except Exception as e:
  #   print(f"Error occurred for H")

  # step 15: apply k means clustering on H
  kmeans = KMeans(n_clusters=k, n_init=10, max_iter=500)
  clusterLabels = kmeans.fit_predict(H)

  end_time = time.time()
  execution_time = end_time - start_time

  return clusterLabels, execution_time, Lnsigma_eigenvalues[1]

"""# Data importing"""

import csv

data = pd.read_csv('./sample_data/lymphography.csv')

data1 = data.iloc[:, 1:]

X = data.iloc[:, 1:].values
y = data.iloc[:, 0].values


pca = PCA(n_components=2)  # Reduce to 2 dimensions
reduced_data = pca.fit_transform(X)


# Plot the reduced data points
plt.figure(figsize=(8, 6))
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap='viridis', alpha=0.7)

# Label the axes (refer to PCA output for component names)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

"""# K nearest neighbors"""

def get_knn(data, point, k):
  knn = NearestNeighbors(n_neighbors=k)
  point = np.array(point)
  knn.fit(data)

  neighbor_distances, neighbor_indices = knn.kneighbors(point)
  return neighbor_indices.flatten(), neighbor_distances.flatten()

"""# Generate mini datasets"""

def generate_mini_datasets(data, k, N):
  knn = NearestNeighbors(n_neighbors=k+1)  # +1 to include the point itself
  knn.fit(data)
  mini_datasets = []
  mini_datasets_features = []
  for i in range(len(data)):
    neighbor_indices, _ = get_knn(data, data[i:i+1], k+1)
    N = min(N, k)
    if N > 0:
      if N > k:
        raise ValueError("N cannot be greater than the number of neighbors (k)")
      else:
        sampled_neighbors = neighbor_indices[1:N+1].astype(int)
    else:
      sampled_neighbors = np.array([])

    # for mini dataset fatures
    central_features = data[i]
    neighbor_features = data[sampled_neighbors]
    mini_dataset_features = np.vstack((central_features, neighbor_features))
    mini_datasets_features.append(mini_dataset_features)
    # for mini dataset
    i_array = np.array([i])
    mini_dataset = np.concatenate(([i_array], sampled_neighbors.reshape(-1, 1)))
    mini_datasets.append(mini_dataset)
  return mini_datasets_features, mini_datasets

data_array = X
mini_datasets_features, mini_datasets = generate_mini_datasets(X, 20, X.shape[0])

print("Number of mini datasets:", len(mini_datasets))

for set1 in mini_datasets:
  print(set1, "\n")

for set2 in mini_datasets_features:
  print(set2, "\n")

# def generate_mini_datasets(data, k, N):

#   knn = NearestNeighbors(n_neighbors=k+1)  # +1 to include the point itself

#   knn.fit(data)

#   mini_datasets = []
#   print(len(data))

#   for i in range(len(data)):
#     neighbor_indices, _ = get_knn(data, data[i:i+1], k+1)

#   # for i in range(len(data)):
#   #   neighbor_indices, _ = knn.kneighbors(data[i:i+1, :])
#   #   neighbor_indices = neighbor_indices.flatten()


#     N = min(N, k)

#     if(N > 0):
#       if(N > k):
#         raise ValueError("N cannot be greater than the number of neighbors (k)")
#       else:
#         sampled_neighbors = neighbor_indices[1:N+1].astype(int)
#     else:
#       sampled_neighbors = np.array([])

#     mini_dataset = np.vstack((data[i], data[sampled_neighbors]))

#     mini_datasets.append(mini_dataset)

#   return mini_datasets



# data_array = data.values
# mini_datasets = generate_mini_datasets(data_array, 5, data_array.shape[0])
# # print(len(mini_datasets))
# # for set1 in mini_datasets:
# #   print(set1, "\n")

"""# Spectral clustering on each mini dataset"""

def get_second_smallest_eigenvalue(mini_datasets_features):
    # print(mini_datasets)
    second_smallest_eigenvalues = []
    for i in range(len(mini_datasets_features)):
      mini_dataset_features = mini_datasets_features[i]
      print("Mini dataset index: ", i, "\n")
      # print("Mini dataset: ", mini_dataset_features)

    # try:
      # check if mini dataset has enough data points for clustering
      if mini_dataset_features.shape[0] < 2:
        print("Skipping mini dataset with less than 2 data points")
        second_smallest_eigenvalues.append(0)
        continue

      # try:
      spectral = SpectralClustering(n_clusters=8)
      spectral.fit(mini_dataset_features)
      eigenvalues, _ = np.linalg.eig(spectral.affinity_matrix_)
      sorted_eigenvalues = np.sort(np.real(eigenvalues))

      if(len(sorted_eigenvalues) > 1):
        second_smallest_eigenvalue = sorted_eigenvalues[1]
        print(second_smallest_eigenvalue)
        second_smallest_eigenvalue = round(second_smallest_eigenvalue, 2)
        second_smallest_eigenvalues.append(second_smallest_eigenvalue)
      else:
        second_smallest_eigenvalues.append(0)

      if(len(second_smallest_eigenvalues) == 0):
        return []

    return second_smallest_eigenvalues


second_smallest_eigenvalues = get_second_smallest_eigenvalue(mini_datasets_features)
print(second_smallest_eigenvalues)

def get_second_smallest_eigenvalue1(mini_datasets_features):
    # print(mini_datasets)
    second_smallest_eigenvalues = []
    for i in range(len(mini_datasets_features)):
      mini_dataset_features = mini_datasets_features[i]
      print("Mini dataset index: ", i, "\n")
      # print("Mini dataset: ", mini_dataset_features)

    # try:
      # check if mini dataset has enough data points for clustering
      if mini_dataset_features.shape[0] < 2:
        print("Skipping mini dataset with less than 2 data points")
        second_smallest_eigenvalues.append(0)
        continue

      # try:
      _, _, eigenvalue = sfsc(mini_dataset_features, 8)
      # spectral.fit(mini_dataset_features)
      # eigenvalues, _ = np.linalg.eig(spectral.affinity_matrix_)
        # _, _, eigenvalue = sfsc(mini_dataset_features, k=2)
        # eigenvalue = round(eigenvalue, 2)
      second_smallest_eigenvalues.append(round(np.real(eigenvalue), 2))
      print(eigenvalue)

      if(len(second_smallest_eigenvalues) == 0):
        return []

    return second_smallest_eigenvalues


second_smallest_eigenvalues = get_second_smallest_eigenvalue1(mini_datasets_features)
print(second_smallest_eigenvalues)

"""# Plot of second smallest eigenvalue Vs mini dataset index"""

def eigenvalue_vs_mini_dataset(second_smallest_eigenvalues):
  x_values = range(1, len(second_smallest_eigenvalues) + 1)

  plt.figure(figsize=(10, 6))
  plt.plot(x_values, second_smallest_eigenvalues, marker='o', linestyle='-')

  plt.title("Plotting of second smallest eigenvalues")
  plt.xlabel("Mini dataset")
  plt.ylabel("Second smallest eigenvalue")

  plt.yticks(np.arange(min(second_smallest_eigenvalues), max(second_smallest_eigenvalues) + 0.05, 0.05))


  plt.show()


eigenvalue_vs_mini_dataset(second_smallest_eigenvalues)

"""# Get top p objects of least frequenct eigenvalue"""

def top_p_least_frequent_indices(second_smallest_eigenvalues, p):
  freq_count = Counter(second_smallest_eigenvalues)
  print(freq_count)
  sorted_freq_count = sorted(freq_count.items(), key=lambda x: x[1])
  least_frequent_values = [value for value, _ in sorted_freq_count[:p]]
  least_frequent_indices = [i for i, value in enumerate(second_smallest_eigenvalues) if value in least_frequent_values]
  return least_frequent_values, least_frequent_indices

p = 9
least_freq_values, least_freq_indices = top_p_least_frequent_indices(second_smallest_eigenvalues, p)
print("Least frequent values:\n", least_freq_values)
print("Corresponding indices:\n", least_freq_indices)
outlier_candidates = least_freq_indices

"""# kth nearest neighbor as the outlier index"""

def outlier_indices(outlier_candidates, mini_datasets, mini_datasets_features, data, k):
  nn_model = NearestNeighbors(n_neighbors=k+1).fit(data)

  outlier_indices = []
  for outlier_candidate in outlier_candidates:
    outlier_candidate_2d = np.atleast_2d(outlier_candidate)

    # if len(mini_dataset.shape) == 1:  # Check if mini_dataset is a single data point
    #   data_without_outlier = [point for point in data if not np.array_equal(point, outlier_candidate)]
    # else:
    data_without_outlier = [point for point in data if not np.array_equal(point, outlier_candidate)]  # Assuming mini-dataset has shape (n_samples, 19)

    mini_dataset_features = mini_datasets_features[outlier_candidate]

    print(mini_dataset_features.shape)

    if len(mini_dataset_features.shape) == 1:
      mini_dataset_features = np.array([mini_dataset_features])

    # mini_dataset_features = mini_dataset_features.reshape(-1, 19)
    distances, _ = nn_model.kneighbors(np.array(mini_dataset_features))
    kth_nn_distance = distances[0][-1]
    outlier_indices.append(kth_nn_distance)

  outlier_tuples = list(zip(outlier_candidates, outlier_indices))
  return outlier_tuples

k = 20


def get_outlier_index(tuple):
    return tuple[1]

outlier_tuples  = outlier_indices(outlier_candidates, mini_datasets, mini_datasets_features, data1, k)
outlier_tuples_desc = sorted(outlier_tuples, key=lambda x: x[1], reverse=True)
print("Outlier indices: \n", outlier_tuples_desc)

"""# Rank outliers"""

def rank_outliers(outlier_tuples_desc, m):
  top_outliers = outlier_tuples_desc[:m]
  final_outliers = [candidate for candidate, index in top_outliers]
  return final_outliers


final_outliers = rank_outliers(outlier_tuples_desc, 7)
print("Final outliers: \n", final_outliers)

"""# Plot the outliers"""

# Extract features and labels
X = data.iloc[:, 1:].values
y = data.iloc[:, 0].values

# Reduce to 2 dimensions using PCA
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(X)

# Plot the reduced data points
plt.figure(figsize=(8, 6))

# Plot normal data points
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, cmap='viridis', alpha=0.7, label='Normal Data')

# Highlight outliers
outlier_indices = final_outliers  # Assuming final_outliers contains indices of outliers
plt.scatter(reduced_data[outlier_indices, 0], reduced_data[outlier_indices, 1], c='red', marker='x', label='Outliers')

# Label the axes
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# Add legend
plt.legend()

# Show plot
plt.show()

